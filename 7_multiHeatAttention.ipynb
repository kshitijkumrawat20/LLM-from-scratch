{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Casual Attention\n",
        "The main difference is that self-attention allows every token in a sequence to attend to every other token, while causal self-attention restricts each token to only attend to tokens that came before it, and itself. Causal self-attention is used for tasks like language modeling, where a model predicts the next word based on only the preceding ones, preventing it from \"seeing\" future information."
      ],
      "metadata": {
        "id": "Z_WGet2rst8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H_samx9jsfcC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43, 0.15, 0.89], # Your\n",
        "        [0.55, 0.87, 0.66], # Journey\n",
        "        [0.57, 0.85, 0.64], # Starts\n",
        "        [0.22, 0.58, 0.33], # with\n",
        "        [0.77, 0.25, 0.10], # one\n",
        "        [0.05, 0.80, 0.55] # step\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    atten_score = query @ key.T\n",
        "    attn_weights = torch.softmax(atten_score/key.shape[-1]**0.5, dim=-1)\n",
        "    context_vec = attn_weights @ value\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "3PmuUI4rtZhi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in = inputs.shape[-1]\n",
        "d_out = 2\n"
      ],
      "metadata": {
        "id": "iwufnn2nt-wp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sa_v2 = SelfAttention_v2(d_in, d_out)"
      ],
      "metadata": {
        "id": "q7ioiLBPtZeA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_score = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohdqPrpGtZbP",
        "outputId": "32fb9446-de72-474c-8104-51b44e073ea1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1504, 0.1662, 0.1663, 0.1741, 0.1702, 0.1729],\n",
              "        [0.1419, 0.1670, 0.1668, 0.1783, 0.1667, 0.1794],\n",
              "        [0.1424, 0.1669, 0.1668, 0.1780, 0.1669, 0.1790],\n",
              "        [0.1516, 0.1672, 0.1670, 0.1736, 0.1658, 0.1748],\n",
              "        [0.1592, 0.1661, 0.1662, 0.1699, 0.1701, 0.1684],\n",
              "        [0.1454, 0.1676, 0.1673, 0.1766, 0.1641, 0.1791]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## generating a mask\n",
        "context_length = attn_score.shape[0]\n",
        "mask = torch.tril(torch.ones(context_length, context_length))\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz1y_ue6tZYi",
        "outputId": "002b352b-3a45-494d-c2a7-64e1115fbd69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now adding this in weights\n",
        "masked_attention_weights = attn_weights * mask\n",
        "masked_attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptm4OrcttZVj",
        "outputId": "45cd4291-c106-4c13-f8ef-8a6dc713afd9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1504, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1419, 0.1670, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1424, 0.1669, 0.1668, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1516, 0.1672, 0.1670, 0.1736, 0.0000, 0.0000],\n",
              "        [0.1592, 0.1661, 0.1662, 0.1699, 0.1701, 0.0000],\n",
              "        [0.1454, 0.1676, 0.1673, 0.1766, 0.1641, 0.1791]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## normalize the weights\n",
        "masked_attention_weights = masked_attention_weights / masked_attention_weights.sum(dim=-1, keepdim=True)\n",
        "masked_attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CCD42omw1IJ",
        "outputId": "949b85e2-2988-494d-a804-ec53f88b31eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4594, 0.5406, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2992, 0.3506, 0.3502, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2299, 0.2535, 0.2533, 0.2632, 0.0000, 0.0000],\n",
              "        [0.1915, 0.1997, 0.1999, 0.2043, 0.2046, 0.0000],\n",
              "        [0.1454, 0.1676, 0.1673, 0.1766, 0.1641, 0.1791]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## then we will multilpy this with value to generate the context vector\n",
        "\n",
        "## but this has a problem because we have already normalized the attention scores which leads to the significant involvement of scores which are masked later causing data leakage problem\n",
        "## other way : attention scores -> upper triangle infinity mask -> normalize(softmax)\n",
        "attn_score\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4nZsFLYxGkN",
        "outputId": "8e0f6c02-5c9f-44a8-f491-b432f2c35360"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2249, -0.0836, -0.0830, -0.0182, -0.0501, -0.0280],\n",
              "        [-0.3269, -0.0969, -0.0985, -0.0044, -0.0990,  0.0045],\n",
              "        [-0.3202, -0.0958, -0.0973, -0.0051, -0.0960,  0.0027],\n",
              "        [-0.1889, -0.0507, -0.0521,  0.0022, -0.0628,  0.0123],\n",
              "        [-0.1086, -0.0491, -0.0480, -0.0166, -0.0150, -0.0296],\n",
              "        [-0.2657, -0.0650, -0.0676,  0.0088, -0.0950,  0.0289]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  ## applying mask\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked_attention_score = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked_attention_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ovrNH_GyaZ9",
        "outputId": "13b93f96-8032-428f-fac1-d85327c7a009"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2249,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.3269, -0.0969,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.3202, -0.0958, -0.0973,    -inf,    -inf,    -inf],\n",
            "        [-0.1889, -0.0507, -0.0521,  0.0022,    -inf,    -inf],\n",
            "        [-0.1086, -0.0491, -0.0480, -0.0166, -0.0150,    -inf],\n",
            "        [-0.2657, -0.0650, -0.0676,  0.0088, -0.0950,  0.0289]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## applying softmax\n",
        "attn_weights= torch.softmax(masked_attention_score/ keys.shape[-1]**0.5,dim = 1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBOE2XCezg4m",
        "outputId": "deb65cdd-2dcd-435e-df2e-2d7d061ad283"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4594, 0.5406, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2992, 0.3506, 0.3502, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2299, 0.2535, 0.2533, 0.2632, 0.0000, 0.0000],\n",
              "        [0.1915, 0.1997, 0.1999, 0.2043, 0.2046, 0.0000],\n",
              "        [0.1454, 0.1676, 0.1673, 0.1766, 0.1641, 0.1791]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## masking addition weights with dropout implemented in GPT models\n",
        "torch.manual_seed(123)\n",
        "dropout= torch.nn.Dropout(0.5)\n",
        "example = torch.ones(6,6)\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtUWOCXQ0U1J",
        "outputId": "35d3d462-b79c-47dc-8226-5d1117a02415"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUX9W3A51bmw",
        "outputId": "1e5b95ab-6fb0-4752-a4f7-5b53cd9b33b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.7012, 0.7005, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.5066, 0.5264, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4092, 0.0000],\n",
              "        [0.2908, 0.3352, 0.0000, 0.3531, 0.3281, 0.3582]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## batching the input\n",
        "batch = torch.stack((inputs, inputs), dim = 0)\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWzChKYn1bjX",
        "outputId": "9856915a-5950-47c1-8a5f-5cf757150f05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]],\n",
              "\n",
              "        [[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86dRWG-91bgi",
        "outputId": "64c91362-d863-4779-cd6c-8e1f860c42cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class CausalAttention_v1(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.dropout= nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length), diagonal = 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, d_in = x.shape ## torch.Size([2, 6, 3])\n",
        "\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    attn_score = query @ key.transpose(1,2) ## here 1 num of tokens and 2 is d_in\n",
        "    attn_score.masked_fill(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens],\n",
        "        -torch.inf\n",
        "    )\n",
        "    attn_weights = torch.softmax(attn_score/key.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = attn_weights @ value\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "Idq5cjFG22CF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "causal_attn = CausalAttention_v1(d_in, d_out, context_length = 6, dropout = 0.5)\n",
        "context_vecs = causal_attn(batch)\n",
        "context_vecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QagOOr2I21-i",
        "outputId": "7fff8240-d03e-47bc-ad6b-7fb3699e1670"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8158, -0.1411],\n",
              "         [-0.6920, -0.0972],\n",
              "         [-0.4050, -0.1201],\n",
              "         [-0.6902, -0.0969],\n",
              "         [-0.5199, -0.0440],\n",
              "         [-0.1417, -0.0505]],\n",
              "\n",
              "        [[-0.7938, -0.2379],\n",
              "         [-0.7858, -0.1145],\n",
              "         [-0.3969,  0.0037],\n",
              "         [-0.7704, -0.2374],\n",
              "         [-0.7801, -0.1107],\n",
              "         [-0.6749, -0.0984]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mutli Head Attention\n",
        "Multi-head attention benefits include the ability to capture different types of relationships and dependencies in the data by attending to different parts of the input in parallel, leading to a more enhanced and contextualized representation. In contrast, single attention forces the model to compress all this information into a single set of weights, limiting its ability to capture complex patterns. The parallel processing in multi-head attention also improves training efficiency."
      ],
      "metadata": {
        "id": "1N0ccRrwIkql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MutliHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_length,dropout,  num_heads, qkv_bias= False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalAttention_v1(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim = -1)"
      ],
      "metadata": {
        "id": "vQDAlHZL213U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MutliHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "context_vecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0vRpBpdPdwe",
        "outputId": "24c7d6e8-277b-4fec-cd24-2aa287e8b3bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.6581,  0.2660,  0.1293, -0.2101],\n",
              "         [-0.6597,  0.2673,  0.1310, -0.2125],\n",
              "         [-0.6594,  0.2670,  0.1310, -0.2125],\n",
              "         [-0.6584,  0.2633,  0.1297, -0.2102],\n",
              "         [-0.6540,  0.2605,  0.1304, -0.2110],\n",
              "         [-0.6610,  0.2659,  0.1297, -0.2104]],\n",
              "\n",
              "        [[-0.6581,  0.2660,  0.1293, -0.2101],\n",
              "         [-0.6597,  0.2673,  0.1310, -0.2125],\n",
              "         [-0.6594,  0.2670,  0.1310, -0.2125],\n",
              "         [-0.6584,  0.2633,  0.1297, -0.2102],\n",
              "         [-0.6540,  0.2605,  0.1304, -0.2110],\n",
              "         [-0.6610,  0.2659,  0.1297, -0.2104]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Multi-Head Attention with Weight splits"
      ],
      "metadata": {
        "id": "2TDkZHNMnuSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MutliHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads) == 0, \\\n",
        "      \"d_out must be divisible by the num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads ## reducing the projection dim to match the desired outpit dim\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out) ## lear layer to combine the head output\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer( ## register_buffer method is used to manage and save tensors that are part of a module's state but are not trainable parameters.When you move your model to a different device (e.g., from CPU to GPU) using model.to(device), any tensors registered as buffers will automatically be moved to that same device\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    )\n",
        "  def forward(self, x): ## lets x = \"the cat sleeps\" and num of head is 2\n",
        "    b,num_tokens, d_in = x.shape\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    ## we implicitly split the matrix by adding a 'num_heads' dimension\n",
        "    keys = key.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = value.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = query.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    ## currently the shape is [1,3,2,3] if the batch size is 1, num of tokens is 3, heads is 2 and head dim is 3\n",
        "    ## so lets transpose it for index 1,2 to make it [1,2,3,3]\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "\n",
        "    ## computing the scaled attention score with causal mask\n",
        "    attn_score = queries @ keys.transpose(2,3)\n",
        "\n",
        "    ## original mask truncated to number of tokens and converted to boolean\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_score.masked_fill_(mask_bool, -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_score/self.head_dim**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    ## shape: (b, num_tokens, num_heads, head_dim)\n",
        "    context_vecs = (attn_weights @ values).transpose(1,2)\n",
        "    context_vecs = context_vecs.contiguous().view(b,num_tokens, self.d_out)\n",
        "    context_vecs= self.out_proj(context_vecs)\n",
        "    return context_vecs"
      ],
      "metadata": {
        "id": "fsoUuugtPdtU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89, 0.55, 0.87, 0.66], # Row 1\n",
        "[0.57, 0.85, 0.64, 0.22, 0.58, 0.33], # Row 2\n",
        "[0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
        ")# Row 3\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "batch_size, context_length, d_in =  batch.shape\n",
        "d_out = 6\n",
        "mha = MutliHeadAttention(d_in, d_out, context_length, 0.0, num_heads = 2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5S02eTU2aBU",
        "outputId": "49cd0ef1-3a4c-4e8e-df23-58910532f542"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    }
  ]
}