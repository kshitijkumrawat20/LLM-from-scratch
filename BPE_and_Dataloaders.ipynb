{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/the-verdict.txt\", encoding =\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of characters:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok-OLJ2Qmi_-",
        "outputId": "61254b65-32de-4959-e05b-b56082eee5b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal: Tokenize all characters\n"
      ],
      "metadata": {
        "id": "n8xv2yT5mqv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text  = \"This is test.\"\n",
        "result = re.split(r'[-,.:;?_!\"()\\[\\]\\s]', text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUrDZ8Etmi8n",
        "outputId": "880a7d64-050c-4fc6-d05a-fa9c75c0cd29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'test', '']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Should remove whitespaces ?\n",
        "1. yes if we want to reduce the memory and computing requirements\n",
        "2. If the text is senstive like python code which have major role of whitespaces in syntax, better to avoid it"
      ],
      "metadata": {
        "id": "x_NDJnEGmxFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## to remove the whitespace\n",
        "result = [item for item in result if item.strip()]\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtVGc7domi5q",
        "outputId": "b2dc0cd7-1626-4110-e4f6-7c5c7978e09b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## processing throught the whole text\n",
        "preprocessed = re.split(r'[-,.:;?_!\"()\\[\\]\\s]',raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cs0x1gmmi2g",
        "outputId": "3e52a459-7598-424c-b5c7-8714852eb757"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', 'though', 'a', 'good', 'fellow', 'enough', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', 'in', 'the', 'height', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## creating Token IDs\n",
        "tokens = sorted(set(preprocessed)) ## sort the words alphabetically\n",
        "tokens.extend([\"<|endoftext|>\",\"<|unk|>\"]) # for the end of one source and unknow token\n",
        "vocab_size = len(tokens)\n",
        "vocab_size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYYatq7Omizh",
        "outputId": "3a1c1469-2800-4a4c-9724-e418a9f6b2eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1154"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## why do we need the vocabulary ?\n",
        "when we convert the ouptut of an LLM into text we need token ids  "
      ],
      "metadata": {
        "id": "GZRBXjs9m505"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "## creating a vocabulary dictionary\n",
        "vocab = {token: integer for integer, token in enumerate(tokens)}\n",
        "for key, value in itertools.islice(vocab.items(),10):\n",
        "      print(f'{key}: {value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0v7bjoDmiwZ",
        "outputId": "4b1fe003-bbbd-46fb-f99b-fcf1d4409875"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "': 0\n",
            "'Are: 1\n",
            "'It's: 2\n",
            "'coming': 3\n",
            "'done': 4\n",
            "'subject: 5\n",
            "'technique': 6\n",
            "'way: 7\n",
            "A: 8\n",
            "Ah: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_inverted = {i:s for s, i in vocab.items()}\n",
        "for key, value in itertools.islice(vocab_inverted.items(),10):\n",
        "      print(f'{key}: {value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxpktf4Omitb",
        "outputId": "e361ad0f-4d5e-427b-bec9-f3cfd0adef71"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: '\n",
            "1: 'Are\n",
            "2: 'It's\n",
            "3: 'coming'\n",
            "4: 'done'\n",
            "5: 'subject\n",
            "6: 'technique'\n",
            "7: 'way\n",
            "8: A\n",
            "9: Ah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'[-,.:;?_!\"()\\[\\]\\s]', text)\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "    def decode(self,ids):\n",
        "        text = \" \".join(self.int_to_str[i] for i in ids)\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text)\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "qVxh2PPum-ln"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer(vocab)\n",
        "\n",
        "text= \"\"\"\"It's the last he painted, you know,kshitij\"\n",
        "      <|endoftext|> Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)## 1154 is kshtitij"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l0QZ9SfnAdm",
        "outputId": "a1e08333-5e9b-4981-c1d9-9feee54d6188"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[59, 1005, 616, 541, 761, 1146, 610, 1153, 1152, 71, 34, 868, 1128, 770, 811]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aPmENHfJoOfo",
        "outputId": "eaf1974a-e355-4646-bfd6-fe747b37c988"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's the last he painted you know <|unk|> <|endoftext|> Mrs Gisburn said with pardonable pride\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Addition content tokens are\n",
        "1. [BOS] [beginning of the sequence]\n",
        "2. [EOS] [End of the sequence]\n",
        "3. [PAD] [Padding]"
      ],
      "metadata": {
        "id": "Ry8TmbvyoyH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT uses BPE techniques\n",
        "Byte-Pair Encoding (BPE) is a data compression and subword tokenization algorithm that creates a new vocabulary by iteratively merging the most frequent pairs of characters in a text. It starts with a vocabulary of individual characters and, through repeated merging, builds a fixed-size vocabulary of subword tokens, allowing models to handle both common words and rare or unseen words efficiently.  "
      ],
      "metadata": {
        "id": "-ooBT13WZlgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using tiktoken library for implementing BPE"
      ],
      "metadata": {
        "id": "ftysoAqCZ4ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_7Y3345ZlTw",
        "outputId": "613566d8-02a7-4442-a649-c0ce015131cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(\"tiktoken version :\", importlib.metadata.version(\"tiktoken\") )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DCJ2ANfZlFr",
        "outputId": "3546a8f0-5edc-4a8b-f936-0efb812de956"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version : 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "llcoBKm5pJ1N"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"Hello do you know about tokenizers? <|endoftext|> I will explaintoyou.\")\n",
        "integers = tokenizer.encode(text, allowed_special= {\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvvJKNcaakt2",
        "outputId": "a66a8611-de67-44b4-9a01-be28dbdc0314"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 466, 345, 760, 546, 11241, 11341, 30, 220, 50256, 314, 481, 1193, 2913, 726, 280, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string = tokenizer.decode(integers)\n",
        "string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DPDFo-Cgakql",
        "outputId": "dfb84dac-3a19-4ab1-e389-9589b13a912f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello do you know about tokenizers? <|endoftext|> I will explaintoyou.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Target Paris\n",
        "We will implement a data loade that fetches the input-target pairs using sliding window approach. We will be using the same dataset and tokenizer.\n"
      ],
      "metadata": {
        "id": "0jiZnI5OkzUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/the-verdict.txt\", \"r\",encoding = \"utf-8\") as file:\n",
        "  text = file.read()\n"
      ],
      "metadata": {
        "id": "KyJ4WLUkakni"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokenizer.encode(text)\n",
        "print(len(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C17xhYVnakkz",
        "outputId": "e7e60ed9-8ede-491a-8cc1-16a2667ae436"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnGviuSxakiJ",
        "outputId": "11b488a4-fb25-42d5-8f36-0e40dad2c6c3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40, 367, 2885, 1464, 1807]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ## the most easiest and intuitive way to create the input-target pairs for the next word prediction task is to x input tokens and y target tokens which are input shifted by 1.\n",
        "\n",
        "## the context size determines how many tokens are included in the input\n",
        "\n",
        "context_size = 4\n",
        "x = encoded_text[:context_size]\n",
        "y = encoded_text[1:context_size + 1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFdXcNcVmfzU",
        "outputId": "c2d994ae-3634-4074-d3ef-417fa15fad94"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [40, 367, 2885, 1464]\n",
            "y: [367, 2885, 1464, 1807]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size + 1):\n",
        "  context = encoded_text[:i]\n",
        "  target = encoded_text[i]\n",
        "  print(f\"x: {context}, --> y: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erobB0FOmfwU",
        "outputId": "899a29f1-0c76-4d10-a4c8-635a24c0b477"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [40], --> y: 367\n",
            "x: [40, 367], --> y: 2885\n",
            "x: [40, 367, 2885], --> y: 1464\n",
            "x: [40, 367, 2885, 1464], --> y: 1807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size + 1):\n",
        "  context = encoded_text[:i]\n",
        "  target = encoded_text[i]\n",
        "  print(f\"x: {tokenizer.decode(context)},--> y: {tokenizer.decode([target])} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7-ep4-krqOZ",
        "outputId": "9a4f5017-a38c-4c2b-a482-452b8ffd5e1c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: I,--> y:  H \n",
            "x: I H,--> y: AD \n",
            "x: I HAD,--> y:  always \n",
            "x: I HAD always,--> y:  thought \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a Data Loader\n",
        "\n",
        "for efficient data loading PyTorch built-in Dataset and Dataloader classes will be used\n",
        "\n",
        "1. Tokenize the entire text\n",
        "2. use a sliding window to chunk the whole book into overlapping sequences of max_length\n",
        "3. Return the total number of rows of dataset\n",
        "4. Return a single row from the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "A3ZzW5lfJOYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "  def __init__(self, text, tokenizer, max_length,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    ## tokenize the entire dataset\n",
        "\n",
        "    token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    ## using sliding window to chunk the book into overlapping sequence\n",
        "    for i in range(0, len(token_ids)- max_length, stride):\n",
        "      input_chunk = token_ids[i:i+ max_length]\n",
        "      target_chunk = token_ids[i+1: i+1 + max_length]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "n0c8J-DJmfqK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## following code will be used for the GPTdataset to load the input in batches via a Pytorch DataLoader.\n",
        "## 1. initalize the tokenizer\n",
        "## 2. create dataset\n",
        "## 3. drop last= True the last which is shorter than batch size is the length is dropped to prevent loss spikes during the training\n",
        "\n",
        "def create_dataloader(text,batch_size= 4, max_length = 256,stride=128, shuffle= True, drop_last = True, num_worker= 0 ):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDataset(text, tokenizer, max_length, stride)\n",
        "  dataLoader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      drop_last= drop_last,\n",
        "      num_workers= num_worker\n",
        "  )\n",
        "  return dataLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8RDMRzimGkK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## convert dataloader into a python iterator to fetch next entry via pythons built-in  next() function\n",
        "\n",
        "import torch\n",
        "dataloader = create_dataloader(\n",
        "    text,\n",
        "    batch_size=1,\n",
        "    max_length=4,\n",
        "    stride=1,\n",
        "    shuffle=False,\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoThNsQZmfnG",
        "outputId": "0ad3a75d-d29e-482f-ae3d-81b1d7f3d3b7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0jb1EvOo1YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ya3D7glbo1Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGsocluNo1Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H41l9D_Do1O5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}