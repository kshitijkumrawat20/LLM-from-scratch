{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Casual Attention\n",
        "The main difference is that self-attention allows every token in a sequence to attend to every other token, while causal self-attention restricts each token to only attend to tokens that came before it, and itself. Causal self-attention is used for tasks like language modeling, where a model predicts the next word based on only the preceding ones, preventing it from \"seeing\" future information."
      ],
      "metadata": {
        "id": "Z_WGet2rst8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_samx9jsfcC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43, 0.15, 0.89], # Your\n",
        "        [0.55, 0.87, 0.66], # Journey\n",
        "        [0.57, 0.85, 0.64], # Starts\n",
        "        [0.22, 0.58, 0.33], # with\n",
        "        [0.77, 0.25, 0.10], # one\n",
        "        [0.05, 0.80, 0.55] # step\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    atten_score = query @ key.T\n",
        "    attn_weights = torch.softmax(atten_score/key.shape[-1]**0.5, dim=-1)\n",
        "    context_vec = attn_weights @ value\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "3PmuUI4rtZhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in = inputs.shape[-1]\n",
        "d_out = 2\n"
      ],
      "metadata": {
        "id": "iwufnn2nt-wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sa_v2 = SelfAttention_v2(d_in, d_out)"
      ],
      "metadata": {
        "id": "q7ioiLBPtZeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_score = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohdqPrpGtZbP",
        "outputId": "efa0b6e2-af58-4be1-bc6c-d10747c6e395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1642, 0.1683, 0.1679, 0.1679, 0.1599, 0.1718],\n",
              "        [0.1669, 0.1667, 0.1664, 0.1678, 0.1616, 0.1706],\n",
              "        [0.1669, 0.1667, 0.1664, 0.1679, 0.1611, 0.1710],\n",
              "        [0.1674, 0.1663, 0.1663, 0.1669, 0.1659, 0.1673],\n",
              "        [0.1660, 0.1674, 0.1666, 0.1694, 0.1541, 0.1765],\n",
              "        [0.1677, 0.1659, 0.1661, 0.1660, 0.1704, 0.1639]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## generating a mask\n",
        "context_length = attn_score.shape[0]\n",
        "mask = torch.tril(torch.ones(context_length, context_length))\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz1y_ue6tZYi",
        "outputId": "52ff6212-d074-46e1-bb81-5b5d0b161f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now adding this in weights\n",
        "masked_attention_weights = attn_weights * mask\n",
        "masked_attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptm4OrcttZVj",
        "outputId": "6777ab21-fde0-4220-86cc-96e36ec48155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1642, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1669, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1669, 0.1667, 0.1664, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1674, 0.1663, 0.1663, 0.1669, 0.0000, 0.0000],\n",
              "        [0.1660, 0.1674, 0.1666, 0.1694, 0.1541, 0.0000],\n",
              "        [0.1677, 0.1659, 0.1661, 0.1660, 0.1704, 0.1639]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## normalize the weights\n",
        "masked_attention_weights = masked_attention_weights / masked_attention_weights.sum(dim=-1, keepdim=True)\n",
        "masked_attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CCD42omw1IJ",
        "outputId": "ec155378-1a0b-4912-ae0a-ac8e355e1fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5004, 0.4996, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3338, 0.3334, 0.3328, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2510, 0.2494, 0.2493, 0.2503, 0.0000, 0.0000],\n",
              "        [0.2016, 0.2032, 0.2023, 0.2057, 0.1872, 0.0000],\n",
              "        [0.1677, 0.1659, 0.1661, 0.1660, 0.1704, 0.1639]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## then we will multilpy this with value to generate the context vector\n",
        "\n",
        "## but this has a problem because we have already normalized the attention scores which leads to the significant involvement of scores which are masked later causing data leakage problem\n",
        "## other way : attention scores -> upper triangle infinity mask -> normalize(softmax)\n",
        "attn_score\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4nZsFLYxGkN",
        "outputId": "b1b67855-479f-4ea7-ef51-541c31b58cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0002,  0.0352,  0.0319,  0.0316, -0.0367,  0.0643],\n",
              "        [-0.0036, -0.0058, -0.0082,  0.0039, -0.0499,  0.0271],\n",
              "        [-0.0039, -0.0053, -0.0080,  0.0049, -0.0539,  0.0303],\n",
              "        [-0.0017, -0.0106, -0.0109, -0.0054, -0.0140, -0.0025],\n",
              "        [-0.0068,  0.0047, -0.0014,  0.0216, -0.1116,  0.0803],\n",
              "        [ 0.0004, -0.0150, -0.0132, -0.0146,  0.0228, -0.0321]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  ## applying mask\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked_attention_score = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked_attention_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ovrNH_GyaZ9",
        "outputId": "ac58d744-e801-46aa-836d-5c9e1437e7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0002,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.0036, -0.0058,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.0039, -0.0053, -0.0080,    -inf,    -inf,    -inf],\n",
            "        [-0.0017, -0.0106, -0.0109, -0.0054,    -inf,    -inf],\n",
            "        [-0.0068,  0.0047, -0.0014,  0.0216, -0.1116,    -inf],\n",
            "        [ 0.0004, -0.0150, -0.0132, -0.0146,  0.0228, -0.0321]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## applying softmax\n",
        "attn_weights= torch.softmax(masked_attention_score/ keys.shape[-1]**0.5,dim = 1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBOE2XCezg4m",
        "outputId": "a1ed93b9-5113-427c-8919-439f066ae938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5004, 0.4996, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3338, 0.3334, 0.3328, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2510, 0.2494, 0.2493, 0.2503, 0.0000, 0.0000],\n",
              "        [0.2016, 0.2032, 0.2023, 0.2057, 0.1872, 0.0000],\n",
              "        [0.1677, 0.1659, 0.1661, 0.1660, 0.1704, 0.1639]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## masking addition weights with dropout implemented in GPT models\n",
        "torch.manual_seed(123)\n",
        "dropout= torch.nn.Dropout(0.5)\n",
        "example = torch.ones(6,6)\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtUWOCXQ0U1J",
        "outputId": "cf7093f0-fbd9-4d7e-8ee0-a5a210e136ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUX9W3A51bmw",
        "outputId": "fa82e750-c7d9-4782-ae6f-8066500c9ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.6669, 0.6656, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.4987, 0.5006, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3744, 0.0000],\n",
              "        [0.3354, 0.3318, 0.0000, 0.3319, 0.3408, 0.3278]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## batching the input\n",
        "batch = torch.stack((inputs, inputs), dim = 0)\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWzChKYn1bjX",
        "outputId": "7af8040b-c873-4923-98f5-f55b8fddf37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]],\n",
              "\n",
              "        [[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86dRWG-91bgi",
        "outputId": "a0dfc76c-15a3-45f1-be9d-757bcea53117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class CausalAttention_v1(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.dropout= nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length), diagonal = 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, d_in = x.shape ## torch.Size([2, 6, 3])\n",
        "\n",
        "    query = self.W_query(x)\n",
        "    key = self.W_key(x)\n",
        "    value = self.W_value(x)\n",
        "\n",
        "    attn_score = query @ key.transpose(1,2) ## here 1 num of tokens and 2 is d_in\n",
        "    attn_score.masked_fill(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens],\n",
        "        -torch.inf\n",
        "    )\n",
        "    attn_weights = torch.softmax(attn_score/key.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = attn_weights @ value\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "Idq5cjFG22CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "causal_attn = CausalAttention_v1(d_in, d_out, context_length = 6, dropout = 0.5)\n",
        "context_vecs = causal_attn(batch)\n",
        "context_vecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QagOOr2I21-i",
        "outputId": "a744acd4-bae6-4f85-879a-539aef5cba11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8158, -0.1411],\n",
              "         [-0.6920, -0.0972],\n",
              "         [-0.4050, -0.1201],\n",
              "         [-0.6902, -0.0969],\n",
              "         [-0.5199, -0.0440],\n",
              "         [-0.1417, -0.0505]],\n",
              "\n",
              "        [[-0.7938, -0.2379],\n",
              "         [-0.7858, -0.1145],\n",
              "         [-0.3969,  0.0037],\n",
              "         [-0.7704, -0.2374],\n",
              "         [-0.7801, -0.1107],\n",
              "         [-0.6749, -0.0984]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQDAlHZL213U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}